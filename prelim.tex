

\section{Multivariable calculus}

A map $\bm{f}\:\RR^n\to\RR^k$ can be thought as array of functions 
\[f_1,\dots,f_k\:\RR^n\to \RR.\]
The map $\bm{f}$ is called \emph{smooth} if each function $f_i$ is smooth;
that is, all partial derivatives of $f_i$ are defined in the domain of definition of $\bm{f}$.

Inverse function theorem gives a sufficient condition for a smooth function to be invertible in a neighborhood of a given point $p$ in its domain.
The condition is formulated in terms of partial derivative of $f_i$ at $p$.

Implicit function theorem is a close relative to inverse function theorem;
in fact it can be obtained as its corollary.
It is used for instance when we need to pass from parametric to implicit description of curves and surface.

Both theorems reduce the existence of a map satisfying certain equation to a question in linear algebra. 
We use these two theorems only for $n\le 3$.

These two theorems are discussed in any course of multivariable calculus, the classical book of Walter Rudin \cite{rudin} is one of my favorites.

\begin{thm}{Inverse function theorem}\label{thm:inverse}
Let $\bm{f}=(f_1,\dots,f_n)\:\RR^n\to\RR^n$ be a smooth map.
Assume that the Jacobian matrix
\[
\begin{pmatrix}
\dfrac{\partial f_1}{\partial x_1} & \cdots & \dfrac{\partial f_1}{\partial x_n}\\
\vdots & \ddots & \vdots\\
\dfrac{\partial f_n}{\partial x_1} & \cdots & \dfrac{\partial f_n}{\partial x_n} \end{pmatrix}\]
is invertible at some point $p$ in the domain of definition of $\bm{f}$.
Then there is a smooth function $\bm{h}\:\RR^m\to\RR^n$ defined is a neighborhood $\Omega_q$ of $q=\bm{f}(p)$ that is \emph{local inverse of $\bm{f}$ at $p$};
that is, there are neighborhoods $\Omega_p\ni p$ such that
$\bm{f}$ defines a bijection $\Omega_p\to \Omega_q$ and
$\bm{f}(x)=y$ if and only if $x=\bm{h}(y)$ for any $x\in \Omega_p$ and any $y\in \Omega_q$.
\end{thm}

\begin{thm}{Implicit function theorem}\label{thm:imlicit}
Let $\bm{f}=(f_1,\dots,f_n)\:\RR^{n+m}\to\RR^n$ be a smooth map,
$m,n\ge 1$.
Let us consider $\RR^{n+m}$ as a product space $\RR^n\times \RR^m$ with coodinates 
$x_1,\dots,x_n,y_1,\dots,y_m$.
Consider the following matrix 
\[
M=\begin{pmatrix}
\dfrac{\partial f_1}{\partial x_1} & \cdots & \dfrac{\partial f_1}{\partial x_n}\\
\vdots & \ddots & \vdots\\
\dfrac{\partial f_n}{\partial x_1} & \cdots & \dfrac{\partial f_n}{\partial x_n} \end{pmatrix}\]
formed by first $n$ columns of the Jacobian matrix.
Assume $M$ is invertible at some point $p$ in the domain of definition of $\bm{f}$ and $\bm{f}(p)=0$.
Then there is a neighborhood $\Omega_p\ni p$
and smooth function $\bm{h}\:\RR^m\to\RR^n$ defined is a neighborhood $\Omega_0\ni 0$ that
for any $(x_1,\dots,x_n,y_1,\dots y_m)\in \Omega_p$ the equality
\[\bm{f}(x_1,\dots,x_n,y_1,\dots y_m)=0\]
holds if and only if 
\[(x_1,\dots x_n)=\bm{h}(y_1,\dots y_m).\]

\end{thm}

If the assumption in the theorem holds for any point $p$ such that $\bm{f}(p)=0$,
then we say that $0$ is a regular value of $\bm{f}$.
The following lemma states that most of the values of smooth map are regular;
in particular generic smooth function satisfies the assumption of the theorem.

\begin{thm}{Sard's lemma}\label{lem:sard}
Almost all values of a smooth map $\bm{f}\colon U \z\to \RR^m$ defined on an open set $U\subset \RR^n$ are regular.
\end{thm}

The words \emph{almost all} means that with exception o set of zero Lebesgue measure.
In particular if one chooses a random value equidistributed in arbitrary small ball $B\subset \RR^m$ then it is a regular value of $f$ with probability 1.

The following theorem is a substitution rule for multiple variables.

\begin{thm}{Theorem}\label{thm:mult-substitution}
Let $K\subset \RR^n$ be a compact set and $h\:K\to\RR$ be a bounded measurable function.
Assume $\bm{f}\:K\to \RR^n$ is an injective smooth map.
Then 
\[\int_K h(\bm{x})\cdot |J_{\bm{f}}(\bm{x})|\cdot d\bm{x}
=
\int_{\bm{f}(K)} h\circ \bm{f}^{-1}(\bm{y})\cdot d\bm{y},\]
where $J_{\bm{f}}(\bm{x})$ denotes the Jacobian of $\bm{f}$ at $\bm{x}$;
that is, the determinant of the Jacobian matrix of $\bm{f}$ at $\bm{x}$.

\end{thm}


\begin{thm}{Area formula}\label{thm:area-formula}
Let $\bm{f}\:K\to\RR^n$ be a smooth map defined on a compact set $K\subset \RR^n$;
denote by $J_{\bm{f}}$ the Jacobian of $\bm{f}$.
Then for any function $h\:K\to \RR$
\[\int_K h(\bm{x})\cdot |J_{\bm{f}}(\bm{x})|\cdot d\bm{x}=\int_{\bm{f}(K)} H_K(\bm{y})\cdot d\bm{y},\]
where 
\[H_K(\bm{y})=\sum_{\substack{\bm{x}\in K \\ \bm{f}(\bm{x})=\bm{y}}}h(\bm{x}).\]
(The integrals are understood in the sense of Lebesgue.)
\end{thm}

Let us sketch the proof of area formula using  Sard's lemma (\ref{lem:sard}) and the substitution rule (\ref{thm:mult-substitution}).

\parit{Sketch of proof.}
Denote by $S\subset K$ the set of critical points of $\bm{f}$; that is, $\bm{x}\in S$ if $J_{\bm{f}}(\bm{x})=0$.
By Sard's lemma,
\emph{$\bm{f}(S)$ has vanishing measure}.
Note that 
\[\int_S h(\bm{x})\cdot |J_{\bm{f}}(\bm{x})|\cdot d\bm{x}=0\]
since $J_{\bm{f}}(\bm{x})=0$
and
\[\int_{\bm{f}(S)} H_S(\bm{y})\cdot d\bm{y}\]
since $\bm{f}(S)$ has vanishing measure.
In particular,
\[\int_S h(\bm{x})\cdot |J_{\bm{f}}(\bm{x})|\cdot d\bm{x}=\int_{\bm{f}(S)} H_S(\bm{y})\cdot d\bm{y};\]
that is the area formula holds for $S$.

It remains to prove that 
\[\int_{K\backslash S} h(\bm{x})\cdot |J_{\bm{f}}(\bm{x})|\cdot d\bm{x}=\int_{\bm{f}(K\backslash S)} H_{K\backslash S}(\bm{y})\cdot d\bm{y}.\]
Since $J_{\bm{f}}(\bm{x})\ne 0$ for any $\bm{x}\in K\backslash S$, by inverse function theorem, the restriction of $\bm{f}$ to a neighborhood $U\ni\bm{x}$ has a smooth inverse.
Therefore for any compact set $K'\subset U$ we have that %???compact
\[\int_{K'}h(\bm{x})\cdot |J_{\bm{f}}(\bm{x})|\cdot d\bm{x}=\int_{\bm{f}(K_1)} h(\bm{f}^-1(\bm{y}))\cdot d\bm{y}.\]
It remains to subdivide $K_1$ into a countable collection of subsets of that type and sum up the corresponding formulas.\qeds

\section{Initial value problem}

The following theorem guarantees existence and uniqueness of a solution of an initial value problem
for a system of ordinary differential equations
\[
\begin{cases}
x_1'(t)&=f_1(x_1,\dots,x_n,t),
\\
&\dots
\\
x_n'(t)&=f_n(x_1,\dots,x_n,t),
\end{cases}
\]
where each $x_i=x_i(t)$ is a real valued function defined on a real interval $\mathbb{I}$
and each $f_i$ is a smooth function defined on $\RR^n\times \mathbb{I}$.

The array functions $(f_1,\dots,f_n)$ can be considered as one vector-valued function 
$\bm{f}\:\RR^n\times \mathbb{I}\to \RR^n$ and the array $(x_1,\dots,x_n)$ can be considered as a vector  $\bm{x}\in\RR^n$.
Therefore the system can be rewritten as one vector equation 
\[\bm{x}'(t)=\bm{f}(\bm{x}, t).\] 

\begin{thm}{Theorem}\label{thm:ODE}
Suppose $\mathbb{I}$ is a real interval and $\bm{f}\:\RR^n\times\mathbb{I}\to \RR^n$ is a smooth function.
Then for any initial data $\bm{x}(t_0)=\bm{u}$ the differential equation 
\[\bm{x}'(t)=\bm{f}(\bm{x},t)\]
has a unique solution $\bm{x}(t)$ defined at a maximal subinterval $\mathbb{J}$ of $\mathbb{I}$ that contains $t_0$.
Moreover
\begin{enumerate}[(a)]
\item  if $\mathbb{J}\ne \mathbb{I}$, that is, if an end $a$ of $\mathbb{J}$ lies in the interior of $\mathbb{I}$, then $\bm{x}(t)$ diverges for $t\to a$;
\item  the function $(\bm{u},t_0,t)\mapsto \bm{x}(t)$ is smooth.
\end{enumerate}


\end{thm}

\section{Real analysis}

Recall that a function $f$ is called Lipschitz if there is a constant $L$ such that 
\[|f(x)-f(y)|\le L\cdot|x-y|\]
for values $x$ and $y$ in the domain of definition of $f$.
This definition works for maps between metric spaces, but we will use it for real-to-real functions only.

\begin{thm}{Rademacher's theorem}\label{thm:rademacher}
Let $f\:[a,b]\to\RR$ be a Lipschitz function.
then derivative $f'(x)$ is defined for alomst all $x\in [a,b]$.
Moreover the derivative $f'$ is a bounded measurable function defined almost everywhere in $[a,b]$ and it satisfies the fundamental theorem of calculus; that is, the following identity 
\[f(b)-f(a)=\int_a^b f'(x)\cdot dx,\]
holds if the integral understood in the sense of Lebesgue.
\end{thm}

It is often helps to work with measurable functions; 
it makes possible to extend many statements about continuous function to measurable functions.

\begin{thm}{Lusin's theorem}\label{thm:lusin}
Let $\phi\:[a,b]\to \RR$ be a measurable function.
Then for any $\eps>0$, there is a continuous function $\psi_\eps\:[a,b]\to \RR$ that coincides with $\phi$ outside of a set of measure at most $\eps$.
Moreover, $\phi$ is bounded above and/or below by some constants then we can assume that so is $\psi_\eps$.  
\end{thm}

\subsection*{Uniform continuity}

Let $f$ be a real function defined on a real interval.
If  for any $\eps>0$ there is $\delta>0$ such that 
\[|x_1-x_2|_X<\delta\quad\Longrightarrow\quad |f(x_1)-f(x_2)|_Y<\eps,\]
then $f$ is called \emph{uniformly continuous}.

Evidently every uniformly continuous function is continuous;
the converse does not hold.
For example, the function $f(x)=x^2$ is continuous, but not uniformly continuous.
Indeed, in this case 
\[|f(x_1)-f(x_2)|=|x_1+x_2|\cdot|x_1-x_2|\eqlbl{eq:ucf}\]
If $|x_1-x_2|$ is arbitrary small positive value, then one is free to chose $ |x_1+x_2|$ sufficiently large, so that the product in \ref{eq:ucf} is bigger that any given number.
However if $f$ is continuous and defined on a closed interval $[a,b]$, then $f$ is uniformly continuous

If the condition above holds for any function $f_n$ in a sequence and $\delta$ depend solely on $\eps$,
then the sequence $(f_n)$ is called uniformly equicontinuous.
More precisely, 
a sequence of functions $f_n:X\to Y$ is called \emph{uniformly equicontinuous} if 
for any $\eps>0$ there is $\delta>0$ such that 
\[|x_1-x_2|_X<\delta\quad\Longrightarrow\quad |f_n(x_1)-f_n(x_2)|_Y<\eps\]
for any $n$.
The following lemma is a partial case of \cite[Theorem 7.25]{rudin}.

\begin{thm}{Lemma}\label{lem:equicontinuous}
Any uniformly equicontinuous sequence of function $f_n\:[a,b]\to [c,d]$ has a subsequence that converges pointwise to a continuous function. 
\end{thm}


\section{Topology}

We sometimes characterization of homeomorphism.

\begin{thm}{Theorem}\label{thm:Hausdorff-compact}
A continuous bijection $f$ between compact metric spaces has continuous inverse;
that is $f$ is a homeomorphism.
\end{thm}


The first part of the following theorem is proved by Camille Jordan, the second part is due to Arthur Schoenflies.

\begin{thm}{Theorem}
The complement of any closed simple plane curve $\gamma$ has exactly two connected components. 

Moreover the there is a homeomorphism $h\:\RR^2\to \RR^2$ that maps the unit circle to $\gamma$.
In particular $\gamma$ bounds a topological disc.
\end{thm}

This theorem is known for simple formulation and quite hard proof.
By now many proofs of this theorem are known.
For the first statement, a very short proof based on somewhat developed technique is given by Patrick Doyle \cite{doyle},
among elementary proofs, one of my favorites is the proof given by Aleksei Filippov \cite{filippov}.

We use the following smooth analog of this theorem.

\begin{thm}{Theorem}
The complement of any closed simple smooth regular plane curve $\gamma$ has exactly two connected components. 

Moreover the there is a diffeomorphism $h\:\RR^2\to \RR^2$ that maps the unit circle to $\gamma$.
\end{thm}

The proof of this statement is much simpler.
An amusing proof of can be found in \cite{chambers-liokumovich}.

\section{Convexity}

A set $X$ in the Euclidean space is called \emph{convex} if for any two points $x,y\in X$, any point $z$ between $x$ and $y$ lies in $X$.
It is called  \emph{strictly convex} if for any two points $x,y\in X$, any point $z$ between $x$ and $y$ lies in the interior of $X$.

From the definition, it is easy to see that intersection of arbitrary family of convex sets is convex. 
The intersection of all convex sets containing $X$ is called \emph{convex hull} of $X$;
it is the minimal convex set containing the given set $X$.


\begin{thm}{Lemma}\label{lem:separation}
Let $K\subset \RR^3$ be a closed convex subset.
Then for any point $p\notin K$ there is a plane $\Pi$ that separates $K$ from $p$;
that is, $K$ and $p$ lie on the opposite open half-spaces of $\Pi$.
\end{thm}

A function of two variables $(x,y)\mapsto f(x,y)$ is called convex if 
its epigraph $z\ge f(x,y)$ is a convex set.
This is equivalent to the so called Jensen's inequality
\[f \left (t\cdot x_1 + (1-t)\cdot x_2 \right ) \leq t\cdot f(x_1)+ (1-t)\cdot f(x_2)\]
for $t\in[0,1]$.
If $f$ is smooth, then the condition is equivalent to the following inequality for second directional derivative:
\[D^2_wf\ge 0\]
for any vector $w\ne 0$ in the $(x,y)$-plane.



\section{Elementary geometry}

\begin{thm}{Theorem}\label{thm:sum=(n-2)pi}
The sum of sum of all the internal angles of a simple $n$-gon is $(n-2)\cdot\pi$. 
\end{thm}


\parit{Proof.}
The proof is by induction on $n$.
For $n=3$ it says that sum of internal angles of a triangle is $\pi$, which is assumed to be known.

First let us show that for any $n\ge4$, any $n$-gon has a diagonal that lies inside of it.
Assume this is holds true for all polygons with at most $n-1$ vertex.

Fix an $n$-gon $P$, $n\ge4$.
Applying rotation if necessary, we can assume that all its vertexes have different $x$-coordinates.
Let $v$ be a vertex of $P$ that minimizes the $x$-coordinate;
denote by $u$ and $w$ its adjacent vertexes.
Let us choose the diagonal $uw$ if it lies in $P$.
Otherwise the triangle $\triangle uvw$ contains another vertex of $P$.
Choose a vertex $s$ in the interior of $\triangle uvw$ that maximizes the distance to line $uw$.
Note that the diagonal $vs$ lyes in $P$;
if it is not the case then $vs$ crosses another side $pq$ of $P$, one of the vertexes $p$ or $q$ has larger distance to the line and it lies in the interior of $\triangle uvw$ --- a contradiction.

Note that the diagonal divides $P$ into two polygons, say $Q$ and $R$, with smaller number of sides in each, say $k$ and $m$ correspondingly.
Note that 
\[k+m=n+2;
\eqlbl{eq:k+m=n+2}\]
indeed each side of $P$ appears once as a side of $P$ or $Q$ plus the diagonal appears twice --- once as a side in $Q$ and once as a side of $R$.
Note that the sum of angles of $P$ is the sum of angles of $Q$ and $R$, which by the induction hypothesis are $(k-2)\cdot\pi$ and $(m-2)\cdot\pi$ correspondingly.
It remains to note that \ref{eq:k+m=n+2} implies
\[(k-2)\cdot\pi+(m-2)\cdot\pi=(n-2)\cdot\pi.\]
\qedsf

The following theorem says that triangle inequality holds for angles between half-lines from a fixed point.
In particular it implies that a unit sphere with angle metric is a metric space.

\begin{thm}{Theorem}\label{thm:spherical-triangle-inq}
The inequality
\[\measuredangle aob+\measuredangle boc\ge\measuredangle aoc\]
holds for any three half-lines $oa$, $ob$ and $oc$ in the Euclidean space.
\end{thm}

The following lemma says that angle of a triangle monotonically depends on the opposit side, assuming the we keep the remaining two sides fixed.
It is a simple statement in elementary geometry; in particular it follows directly from the cosine rule.

\begin{thm}{Lemma}\label{lem:angle-monotonicity}
Let $x$, $y$, $z$, $x'$, $y'$ and $z'$ be 6 points such that $|x-y|=|x'-y'|>0$ and $|y-z|=|y'-z'|>0$.
Then 
\[\measuredangle xyz\ge \measuredangle x'y'z'
\quad\text{if and only if}\quad
|x-z|\ge |x'-y'|.\]
\end{thm}

\begin{wrapfigure}{o}{30 mm}
\vskip-0mm
\centering
\includegraphics{mppics/pic-69}
\vskip0mm
\end{wrapfigure}

\parit{Proof of \ref{thm:spherical-triangle-inq}.}
We can assume that $\measuredangle aob<\measuredangle aoc$; otherwise the statement is evident.
In this case there is a half-line $ob'$ in the angle $aoc$ such that 
\[\measuredangle aob=\measuredangle aob',\]
so in particular we have that
\[\measuredangle aob'+\measuredangle b'oc=\measuredangle aoc.\]
Without loss of generality we can assume that  $|o-b|=|o-b'|$ and $b'$ lies on a line segment $ac$, so
\[|a-b'|+|b'-c|=|a-c|.\]

Then by triangle inequality 
\[
\begin{aligned}
|a-b|+|b-c|&\ge |a-c|=
\\
&=|a-b'|+|b'-c|.
\end{aligned}
\eqlbl{eq:triangle-inq-abcb'}
\]

Note that in the triangles $aob$ and $aob'$ the side $ao$ is shared, $\measuredangle aob\z=\measuredangle aob'$ and $|o-b|=|o-b'|$.
By side-angle-side congruence condition, we have that $\triangle aob\cong \triangle aob'$;
in particular $|a-b'|=|a-b|$.
Therefore from \ref{eq:triangle-inq-abcb'} we have that 
\[|b-c|\ge |b'-c|.\]
Applying the angle monotonicity (\ref{lem:angle-monotonicity}) we get that
\[\measuredangle boc\ge \measuredangle b'oc.\]
Whence
\begin{align*}
\measuredangle aob+\measuredangle boc
&\ge \measuredangle aob'+\measuredangle b'oc=
\\
&=\measuredangle aoc.
\end{align*}
\qedsf
