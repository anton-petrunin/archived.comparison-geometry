\section{Linear algebra}

\begin{thm}{Spectral theorem}\label{thm:spectral}
Any symmetric matrix is diagonalizable  by orthogonal matrix.
\end{thm}

We will use this theorem only for $2{\times}2$ matrices.
In this case it can be restated as follows:
Consider a function 
\[f(x,y)=\ell\cdot x^2+2\cdot m\cdot x\cdot y+n\cdot y^2,\]
that is defined on a $(x,y)$-coordinate plane.
Then after proper rotation of the coordinates, one can assume that $m=0$. %???REF


\section{Multivariable calculus}

This material is discussed in any course of multivariable calculus, the classical book of Walter Rudin \cite{rudin} is one of our favorites.

\subsection*{Regular value}

A map $\bm{f}\:\RR^m\to\RR^n$ can be thought as an array of functions 
\[f_1,\dots,f_n\:\RR^m\to \RR.\]
The map $\bm{f}$ is called \index{smooth map}\emph{smooth} if each function $f_i$ is smooth;
that is, all partial derivatives of $f_i$ are defined in the domain of definition of $\bm{f}$.

The Jacobian matrix of $\bm{f}$ at $\bm{x}\in\RR^m$ is defined as
\[\Jac_{\bm{x}}\bm{f}=
\begin{pmatrix}
\dfrac{\partial f_1}{\partial x_1} & \cdots & \dfrac{\partial f_1}{\partial x_m}\\
\vdots & \ddots & \vdots\\
\dfrac{\partial f_n}{\partial x_1} & \cdots & \dfrac{\partial f_n}{\partial x_m} \end{pmatrix};\]\index{$\Jac_{\bm{x}}\bm{f}$}
we assume that the right hand side is evaluated at $\bm{x}=(x_1,\dots,x_m)$.

If the Jacobean matrix defines a surjective linear map $\RR^m\to\RR^n$ (that is, if $\rank(\Jac_{\bm{x}}\bm{f})=n$) then we say that 
$\bm{x}$ is a \index{regular point}\emph{regular point} of~$\bm{f}$.

If for some $\bm{y}\in \RR^n$ each point $\bm{x}$ such that $\bm{f}(\bm{x})=\bm{y}$ is regular,
then we say that $\bm{y}$ is a \index{regular value}\emph{regular value} of $\bm{f}$.
The following lemma states that {}\emph{most} values of a smooth map are regular.

\begin{thm}{Sard's lemma}\label{lem:sard}
Given a smooth map $\bm{f}\colon \Omega \z\to \RR^n$ defined on an open set $\Omega\subset \RR^m$, almost all values in $\RR^n$ are regular.
\end{thm}

The words \index{almost all}\emph{almost all} means all values, with the possible exceptions belong to a set with vanishing {}\emph{Lebesgue measure}.
In particular if one chooses a random value equidistributed in an arbitrarily small ball $B\subset \RR^n$, then it is a regular value of $\bm{f}$ with probability 1.

Note that if $m<n$, then $\bm{f}$ has no regular points.
Therefore the only regular value of $\bm{f}$ are the points in the complement of the image $\Im \bm{f}$.
The theorem states that in this case almost all points in $\RR^n$, do {}\emph{not} belong to $\Im \bm{f}$.


\subsection*{Inverse function theorem}

The \index{inverse function theorem}\emph{inverse function theorem} gives a sufficient condition for a smooth map $\bm{f}$ to be invertible in a neighborhood of a given point $\bm{x}$.
The condition is formulated in terms of the Jacobian matrix of $\bm{f}$ at $\bm{x}$.

The \index{implicit function theorem}\emph{implicit function theorem} is a close relative to the inverse function theorem;
in fact it can be obtained as its corollary.
It is used when we need to pass from parametric to implicit description of curves and surfaces.

Both theorems reduce the existence of a map satisfying certain equation to a question in linear algebra.
We use these two theorems only for $n\le 3$.

\begin{thm}{Inverse function theorem}\label{thm:inverse}
Let $\bm{f}=(f_1,\dots,f_n)\:\Omega\to\RR^n$ be a smooth map
defined on an open set $\Omega\subset \RR^n$.
Assume that the Jacobian matrix
$\Jac_{\bm{x}}\bm{f}$
is invertible at some point $\bm{x}\in \Omega$.
Then there is a smooth map $\bm{h}\:\Phi\to\RR^n$ defined in an open neighborhood $\Phi$ of ${\bm{y}}\z=\bm{f}(\bm{x})$ that is a {}\emph{local inverse of $\bm{f}$ at $\bm{x}$};
that is, there is a neighborhood $\Psi\ni \bm{x}$ such that
$\bm{f}$ defines a bijection $\Psi\leftrightarrow \Phi$ and
$\bm{h} \circ \bm{f}$ is an identity map on $\Psi$.

Moreover if an $\Omega$ contains an $\eps$-neighborhood of $\bm{x}$, and the first and second partial derivatives $\tfrac{\partial f_i}{\partial x_j}$, $\tfrac{\partial^2 f_i}{\partial x_j\partial x_k}$ are bounded by a constant $C$ for all $i$, $j$, and $k$, then we can assume that $\Phi$ is a $\delta$-neighborhood of $\bm{y}$, for some $\delta>0$ that depends only on $\eps$ and $C$. 
\end{thm}

\begin{thm}{Implicit function theorem}\label{thm:imlicit}
Let $\bm{f}=(f_1,\dots,f_n)\:\Omega\to\RR^n$ be a smooth map, defined on a open subset $\Omega\subset\RR^{n+m}$, where
$m,n\ge 1$.
Let us consider $\RR^{n+m}$ as a product space $\RR^n\times \RR^m$ with coodinates 
$x_1,\dots,x_n,y_1,\dots,y_m$.
Consider the following matrix 
\[
M=\begin{pmatrix}
\dfrac{\partial f_1}{\partial x_1} & \cdots & \dfrac{\partial f_1}{\partial x_n}\\
\vdots & \ddots & \vdots\\
\dfrac{\partial f_n}{\partial x_1} & \cdots & \dfrac{\partial f_n}{\partial x_n} \end{pmatrix}\]
formed by the first $n$ columns of the Jacobian matrix.
Assume $M$ is invertible at some point $\bm{x}=(x_1,\dots,x_n,y_1,\dots y_m)$ in the domain of definition of $\bm{f}$ and $\bm{f}(\bm{x})=0$.
Then there is a neighborhood $\Psi\ni \bm{x}$
and a smooth function $\bm{h}\:\RR^m\to\RR^n$ defined in a neighborhood $\Phi\ni 0$ such that
for any $(x_1,\dots,x_n,y_1,\dots y_m)\in \Omega$, the equality
\[\bm{f}(x_1,\dots,x_n,y_1,\dots y_m)=0\]
holds if and only if 
\[(x_1,\dots x_n)=\bm{h}(y_1,\dots y_m).\]

\end{thm}

\subsection*{Multiple integral}

Set 
\[\jac_{\bm{x}}\bm{f}\df|\det[\Jac_{\bm{x}}\bm{f}]|;\index{$\jac_{\bm{x}}\bm{f}$}\]
that is, $\jac_{\bm{x}}\bm{f}$ is the absolute value of the determinant of the Jacobian matrix of $\bm{f}$ at $\bm{x}$.

The following theorem plays the role of a substitution rule for multiple variables.

\begin{thm}{Theorem}\label{thm:mult-substitution}
Let $h\:K\to\RR$ be a bounded measurable function on a measurable subset $K\subset \RR^n$.
Assume $\bm{f}\:K\to \RR^n$ is an injective smooth map.
Then 
\[\idotsint_{\bm{x}\in K} h(\bm{x})\cdot \jac_{\bm{x}}\bm{f}
=
\idotsint_{\bm{y}\in \bm{f}(K)} h\circ \bm{f}^{-1}(\bm{y}).\]

\end{thm}




\section{Ordinary differential equations}

\subsection*{Systems of first order}

The following theorem guarantees existence and uniqueness of solutions of an initial value problem
for a system of ordinary first order differential equations
\[
\begin{cases}
x_1'&=f_1(x_1,\dots,x_n,t),
\\
&\,\,\vdots
\\
x_n'&=f_n(x_1,\dots,x_n,t),
\end{cases}
\]
where each $t\mapsto x_i=x_i(t)$ is a real valued function defined on a real interval $\mathbb{I}$
and each $f_i$ is a smooth function defined on an open subset $\Omega\subset \RR^n\times \RR$.

The array of functions $(f_1,\dots,f_n)$ can be considered as one vector-valued function 
$\bm{f}\:\Omega\to \RR^n$ and the array $(x_1,\dots,x_n)$ can be considered as a vector  $\bm{x}\in\RR^n$.
Therefore the system can be rewritten as one vector equation 
\[\bm{x}'=\bm{f}(\bm{x}, t).\] 

\begin{thm}{Theorem}\label{thm:ODE}
Suppose $\mathbb{I}$ is a real interval and $\bm{f}\:\Omega\to \RR^n$ is a smooth function defined on an open subset $\Omega\subset \RR^n\times \RR$.
Then for any initial data $\bm{x}(t_0)=\bm{u}$ such that $(\bm{u},t)\in\Omega$ the differential equation 
\[\bm{x}'=\bm{f}(\bm{x},t)\]
has a unique solution $t\mapsto \bm{x}(t)$ defined at a maximal interval $\mathbb{J}$ that contains $t_0$.
Moreover
\begin{enumerate}[(a)]
\item  if $\mathbb{J}\ne \RR$ (that is, if an end $a$ of $\mathbb{J}$ is finite) then $\bm{x}(t)$ does not have a limit point in $\Omega$ as $t\to a$;
\item  the function $(\bm{u},t_0,t)\mapsto \bm{x}(t)$ has open domain of definition in $\Omega\times \RR$ and it is smooth in this domain.
\end{enumerate}

\end{thm}

\subsection*{Higher order}

Suppose we have an ordinary differential equation of order $k$
\[\bm{x}^{(k)}=\bm{f}(\bm{x},\dots,\bm{x}^{(k-1)},t),\]
where $\bm{x}=\bm{x}(t)$ is a function from a real inerval to $\RR^n$.

This equation can be rewritten as $k$ first order equations as follows with $k-1$ new variables $\bm{y}_i=\bm{x}^{(i)}$:
\[
\begin{cases}
\bm{x}'&=\bm{y}_1
\\
\bm{y}_1'&=\bm{y}_2
\\
&\,\,\vdots
\\
\bm{y}_{k-1}'(t)&=\bm{f}(\bm{x},\bm{y}_{1},\dots,\bm{y}_{k-1},t),
\end{cases}
\]

Using this trick one can reduce a higher order ordinary differential equation to a first order equation. 
In particular we get local existence and uniqueness for solutions of higher order equations as in Theorem \ref{thm:ODE}.

\section{Analysis}\label{sec:analysis}

\subsection*{Lipschitz condition}

Recall that a function $f$ between metric spaces is called \index{Lipschitz function}\emph{Lipschitz} if there is a constant $L$ such that 
\[|f(x)-f(y)|\le L\cdot|x-y|\]
for all values $x$ and $y$ in the domain of definition of $f$.
%Although this definition makes sense for maps between metric spaces, we will only use it for real-to-real functions.

The following theorem makes possible to extend number of results about smooth function to Lipschitz functions.
Recall that {}\emph{almost all} means all values, with the possible exceptions belong to a set with vanishing {}\emph{Lebesgue measure}.

\begin{thm}{Rademacher's theorem}\label{thm:rademacher}
Let $f\:[a,b]\to\RR$ be a Lipschitz function.
Then the derivative $f'$ of $f$ is a bounded measurable function defined almost everywhere in $[a,b]$ and it satisfies the fundamental theorem of calculus; that is, the following identity 
\[f(b)-f(a)=\int_a^b f'(x)\cdot dx,\]
holds if the integral is understood in the sense of Lebesgue.
\end{thm}

The following theorem makes possible to extend many statements about continuous function to measurable functions.

\begin{thm}{Lusin's theorem}\label{thm:lusin}
Let $\phi\:[a,b]\to \RR$ be a measurable function.
Then for any $\eps>0$, there is a continuous function $\psi_\eps\:[a,b]\to \RR$ that coincides with $\phi$ outside of a set of measure at most $\eps$.
Moreover, if $\phi$ is bounded above and/or below by some constants, then we may assume that so is $\psi_\eps$.  
\end{thm}

\subsection*{Convex functions}

A function of two variables $(x,y)\mapsto f(x,y)$ is called \index{convex function}\emph{convex} if 
its epigraph $z\ge f(x,y)$ is a convex set.
This is equivalent to the so-called \index{Jensen's inequality}\emph{Jensen's inequality}
\[f \left (t\cdot x_1 + (1-t)\cdot x_2 \right ) \leq t\cdot f(x_1)+ (1-t)\cdot f(x_2)\]
for $t\in[0,1]$.
Moreover, 
\begin{itemize}
\item it is sufficient to check Jensen's inequality assuming that the distance $|x_1-x_2|$ is sufficiently small.
\item if $f$ is continuous, then it is sufficient to check Jensen's inequality for $t=\tfrac12$.
\end{itemize}



If $f$ is smooth, then the condition is equivalent to the following inequality for the second directional derivative:
\[D^2_{\vec w}f\ge 0\]
for any vector $\vec w$ in the $(x,y)$-plane.

\subsection*{Uniform continuity and convergence}

Let $f\:{\spc{X}}\to \spc{Y}$ be a map between metric spaces.
If  for any $\eps>0$ there is $\delta>0$ such that 
\[|x_1-x_2|_{\spc{X}}<\delta\quad\Longrightarrow\quad |f(x_1)-f(x_2)|_\spc{Y}<\eps,\]
then $f$ is called \index{uniformly continuous}\emph{uniformly continuous}.

Evidently every uniformly continuous function is continuous;
the converse does not hold.
For example, the function $f(x)=x^2$ is continuous, but not uniformly continuous.

However if $f$ is continuous and defined on a closed interval $[a,b]$, then $f$ is uniformly continuous.

If the condition above holds for any function $f_n$ in a sequence and $\delta$ depend solely on $\eps$,
then the sequence $(f_n)$ is called \index{uniformly continuous}\emph{uniformly equicontinuous}.
More precisely, 
a sequence of functions $f_n:{\spc{X}}\to \spc{Y}$ is called {}\emph{uniformly equicontinuous} if 
for any $\eps>0$ there is $\delta>0$ such that 
\[|x_1-x_2|_{\spc{X}}<\delta\quad\Longrightarrow\quad |f_n(x_1)-f_n(x_2)|_\spc{Y}<\eps\]
for any $n$.


We say that a sequence of functions $f_i : {\spc{X}} \to \spc{Y}$ \index{uniform convergence}\emph{converges uniformly} to a function $f_{\infty}: {\spc{X}} \to \spc{Y}$ if for any 
$\varepsilon >0$, there is a natural number $N$ such that for all $n \geq N$, we have $| f_{\infty} (x)- f_n (x) | < \varepsilon$
for all $x  \in {\spc{X}}$.

\begin{thm}{Arzel\'{a}--Ascoli Theorem}\label{lem:equicontinuous}
Suppose $\spc{X}$ and $\spc{Y}$ are compact metric spaces. 
Then any uniformly equicontinuous sequence of function $f_n\:\spc{X}\to \spc{Y}$ has a subsequence that converges uniformly to a continuous function $f_\infty\:\spc{X}\to \spc{Y}$. 
\end{thm}

\subsection*{Cutoffs and mollifiers}

We use few examples of smooth functions that mimic behavior of some model functions.

For example, consider the following functions
\begin{align*}
h(t)&=
\begin{cases}
0&\text{if}\ t\le 0,
\\
t&\text{if}\ t> 0.
\end{cases}
&
f(t)&=
\begin{cases}
0&\text{if}\ t\le 0,
\\
\frac{t}{e^{1\!/\!t}}&\text{if}\ t> 0.
\end{cases}
\end{align*}
\begin{figure}[h]
\begin{minipage}{.48\textwidth}
\centering
\includegraphics{mppics/pic-320}
\end{minipage}\hfill
\begin{minipage}{.48\textwidth}
\centering
\includegraphics{mppics/pic-321}
\end{minipage}
\end{figure}
Note that $h$ and $f$ behave alike ---
both vanish at $t\le 0$ and grows to infinity for positive $t$.
The function $h$ is not smooth --- its derivative at $0$ is undefined.
Unlike $h$, the function $f$ is smooth.
Indeed, the existence of all derivatives $f^{(n)}(x)$ at $x\ne 0$ is evident and direct calculations show that $f^{(n)}(0)=0$ for all $n$.

Other useful examples of that type are the so-called \index{bell function}\emph{bell function} --- a smooth function that is positive in an $\eps$-neighborhood of zero and vanishing outside this neighborhood.
An example of such function can be constructed based using the function $f$ constructed above, say 
\[b_\eps(t)=c\cdot f(\eps^2-t^2);\]
the constant $c$ is chosen so that $\int b_\eps=1$.

\begin{figure}[h!]
\begin{minipage}{.48\textwidth}
\centering
\includegraphics{mppics/pic-325}
\end{minipage}\hfill
\begin{minipage}{.48\textwidth}
\centering
\includegraphics{mppics/pic-326}
\end{minipage}
\end{figure}

Another useful example is a sigmoid --- nondecreasing function that vanish for $t\le -\eps$ and takes value $1$ for any $t\ge \eps$.
For example the following function \label{page:sigma-function}
\[\sigma_\eps(t)
=
\int_{-\infty}^t b_\eps(x)\cdot dx.\]

%???Borel sets

\section{Topology}

The following material is covered in any reasonable introductory text to topology; 
one of our favorites is a textbook of Czes Kosniowski \cite{kosniowski}.

\subsection*{Continuous inverse}

We sometimes use the following characterization of homeomorphisms between compact spaces.

\begin{thm}{Theorem}\label{thm:Hausdorff-compact}
A continuous bijection $f$ between compact metric spaces has a continuous inverse;
that is, $f$ is a homeomorphism.
\end{thm}

\subsection*{Jordan's theorem}
\index{Jordan's theorem}

The first part of the following theorem was proved by Camille Jordan, the second part is due to Arthur Schoenflies.

\begin{thm}{Theorem}\label{thm:jordan}
The complement of any closed simple plane curve $\gamma$ has exactly two connected components. 

Moreover, there is a homeomorphism $h\:\RR^2\to \RR^2$ that maps the unit circle to $\gamma$.
In particular $\gamma$ bounds a topological disc.
\end{thm}

This theorem is known for its simple formulation and quite hard proof.
By now many proofs of this theorem are known.
For the first statement, a very short proof based on a somewhat developed technique is given by Patrick Doyle \cite{doyle},
among elementary proofs, one of my favorites is the proof given by Aleksei Filippov \cite{filippov}.

We use the following smooth analog of this theorem.

\begin{thm}{Theorem}
The complement of any closed simple smooth regular plane curve $\gamma$ has exactly two connected components. 

Moreover the there is a diffeomorphism $h\:\RR^2\to \RR^2$ that maps the unit circle to $\gamma$.
\end{thm}

The proof of this statement is much simpler.
An amusing proof can be found in \cite{chambers-liokumovich}.

\subsection*{Connectedness}

Recall that a continuous map $\alpha$ from the unit interval $[0,1]$ to a Euclidean space is called a \index{path}\emph{path}.
If $p=\alpha (0)$ and $q = \alpha (1)$, then we say that $\alpha$ connects $p$ to $q$.


A set $X$ in the Euclidean space is called \index{path connected set}\emph{path connected} if any two points $x,y\in X$ can be connected by a path lying in $X$.

A set $X$ in the Euclidean space is called \index{connected set}\emph{connected} if one cannot cover $X$ with two disjoint open sets $V$ and $W$ such that both intersections $X\cap V$ and $X\cap W$ are nonempty.

\begin{thm}{Proposition}
Any path connected set is connected.
Moreover, any open connected set in the Euclidean space or  plane is path connected.
\end{thm}

Given a point $x\in X$, the maximal connected subset of $X$ containing $x$ is called the \index{connected component}\emph{connected component} of $x$ in $X$.



\section{Elementary geometry}

\subsection*{Polygons}

\begin{thm}{Theorem}\label{thm:sum=(n-2)pi}
The sum of all the internal angles of a simple $n$-gon is $(n-2)\cdot\pi$. 
\end{thm}

While this theorem is well known, it is hard to find a reference with a proof without cheating.
So we present a proof here.

A triangle with vertexes $x$, $y$, and $z$ will be denoted by $[xyz]$.

\parit{Proof.}
The proof is by induction on $n$.
For $n=3$ it says that sum of internal angles of a triangle is $\pi$, which is assumed to be known.

First let us show that for any $n\ge4$, any $n$-gon has a diagonal that lies inside of it.
Assume this holds true for all polygons with at most $n-1$ vertices.

Fix an $n$-gon $P$, $n\ge4$.
Applying a rotation if necessary, we can assume that all its vertexes have different $x$-coordinates.
Let $v$ be a vertex of $P$ that minimizes the $x$-coordinate;
denote by $u$ and $w$ its adjacent vertexes.
Let us choose the diagonal $uw$ if it lies in $P$.
Otherwise the triangle $[uvw]$ contains another vertex of $P$.
Choose a vertex $s$ in the interior of the triangle $[uvw]$ that maximizes the distance to line $uw$.
Note that the diagonal $vs$ lies in $P$;
if it is not the case, then $vs$ crosses another side $pq$ of $P$, one of the vertices $p$ or $q$ has larger distance to the line and it lies in the interior of the triangle $[uvw]$ --- a contradiction.

Note that the diagonal divides $P$ into two polygons, say $Q$ and $R$, with smaller number of sides in each, say $k$ and $m$ respectively.
Note that 
\[k+m=n+2;
\eqlbl{eq:k+m=n+2}\]
indeed each side of $P$ appears once as a side of $P$ or $Q$ plus the diagonal appears twice --- once as a side in $Q$ and once as a side of $R$.

Observe that $Q$ and $R$, which by the induction hypothesis are $(k\z-2)\cdot\pi$ and $(m-2)\cdot\pi$ respectively,
and the sum of angles of $P$ equals to the sum of angles in $Q$ and $Q$.
Finally by \ref{eq:k+m=n+2} we have that
\[(k-2)\cdot\pi+(m-2)\cdot\pi=(n-2)\cdot\pi\]
which proves the theorem.
\qeds

\subsection*{Triangle inequality for angles}

The \index{angle measure}\emph{measure} of angle with sides $[px]$ and $[py]$ will be denoted by $\measuredangle\hinge pxy$\index{$\measuredangle\hinge pxy$};
it takes a value in the interval $[0,\pi]$.

The following theorem says that the triangle inequality holds for angles between half-lines from a fixed point.
In particular it implies that a sphere with the angle metric is a metric space.

\begin{thm}{Theorem}\label{thm:spherical-triangle-inq}
The inequality
\[\measuredangle\hinge oab
+
\measuredangle\hinge obc
\ge
\measuredangle\hinge oac\]
holds for any three line segments $[oa]$, $[ob]$ and $[oc]$ in the Euclidean space.
\end{thm}

We will use the following lemma in the proof;
it says that the angle of a triangle monotonically depends on the opposite side, assuming the we keep the other two sides fixed.
It follows directly from the cosine rule.

\begin{thm}{Lemma}\label{lem:angle-monotonicity}
Let $x$, $y$, $z$, $x^{*}$, $y^{*}$ and $z^{*}$ be 6 points such that $|x-y|\z=|x^{*}-y^{*}|>0$ and $|y-z|=|y^{*}-z^{*}|>0$.
Then 
\[\measuredangle\hinge yxz
\ge
\measuredangle\hinge {y^{*}}{x^{*}}{z^{*}}
\quad\text{if and only if}\quad
|x-z|\ge |x^{*}-z^{*}|.\]
\end{thm}

\begin{wrapfigure}{r}{30 mm}
\vskip-0mm
\centering
\includegraphics{mppics/pic-69}
\vskip4mm
\end{wrapfigure}

\parit{Proof of \ref{thm:spherical-triangle-inq}.}
We can assume that 
$\measuredangle\hinge oab<
 \measuredangle\hinge oac$; otherwise the statement is evident.
In this case there is a half-line $ob^{*}$ in the angle $aoc$ such that 
\[\measuredangle\hinge oab
=\measuredangle\hinge oa{b^{*}},\]
so in particular we have that
\[
\measuredangle\hinge oa{b^{*}}+
\measuredangle\hinge o{b^{*}}c=
\measuredangle\hinge oac.\]
Without loss of generality we can assume that  $|o-b|=|o-b^{*}|$ and $b^{*}$ lies on a line segment $ac$, so
\[|a-b^{*}|+|b^{*}-c|=|a-c|.\]

Then by the triangle inequality 
\[
\begin{aligned}
|a-b|+|b-c|&\ge |a-c|=
\\
&=|a-b^{*}|+|b^{*}-c|.
\end{aligned}
\eqlbl{eq:triangle-inq-abcb'}
\]

Note that in the triangles $aob$ and $aob^{*}$ the side $ao$ is shared, so 
$
\measuredangle\hinge oab\z=
\measuredangle\hinge oab^{*}$ and $|o-b|=|o-b^{*}|$.
By side-angle-side congruence condition, we have that $[aob]\cong [aob^{*}]$;
in particular $|a-b^{*}|=|a-b|$.
Therefore from \ref{eq:triangle-inq-abcb'} we get 
\[|b-c|\ge |b^{*}-c|.\]
Applying the angle monotonicity (\ref{lem:angle-monotonicity}) we obtain
\[
\measuredangle\hinge obc\ge 
\measuredangle\hinge o{b^{*}}c.\]
Whence
\begin{align*}
\measuredangle\hinge oab+
\measuredangle\hinge obc
&\ge 
\measuredangle\hinge oa{b^{*}}+
\measuredangle\hinge o{b^{*}}c=
\\
&=
\measuredangle\hinge oac.
\end{align*}
\qedsf

\subsection*{Convex sets}

A set $X$ in the Euclidean space is called \index{convex set}\emph{convex} if for any two points $x,y\in X$, any point $z$ between $x$ and $y$ lies in $X$.
It is called  \index{strictly convex set}\emph{strictly convex} if for any two points $x,y\in X$, any point $z$ between $x$ and $y$ lies in the interior of $X$.

From the definition, it is easy to see that the intersection of an arbitrary family of convex sets is convex. 
The intersection of all convex sets containing $X$ is called the \index{convex hull}\emph{convex hull} of $X$;
it is the minimal convex set containing the set $X$.

We will use the following corollary of the so-called \index{hyperplane separation theorem}\emph{hyperplane separation theorem}:

\begin{thm}{Lemma}\label{lem:separation}
Let $K\subset \RR^3$ be a closed convex set.
Then for any point $p\notin K$ there is a plane $\Pi$ that separates $K$ from $p$;
that is, $K$ and $p$ lie on opposite open half-spaces separated by $\Pi$.
\end{thm}

\section{Area}

\subsection*{Area of spherical triangle}

\begin{thm}{Lemma}\label{lem:area-spher-triangle}
Let $\Delta$ be a spherical triangle;
that is, $\Delta$ is the intersection of three closed half-spheres in the unit sphere $\mathbb{S}^2$.
Then 
\[\area\Delta=\alpha+\beta+\gamma-\pi,\eqlbl{eq:area(Delta)}\]
where $\alpha$, $\beta$ and $\gamma$ are the angles of $\Delta$.
\end{thm}

The value $\alpha+\beta+\gamma-\pi$ is called \index{excess of triangle}\emph{excess of the triangle} $\Delta$.

\begin{wrapfigure}{r}{22 mm}
\vskip-0mm
\centering
\includegraphics{mppics/pic-43}
\vskip2mm
\end{wrapfigure}

\parit{Proof.}
Recall that 
\[\area\mathbb{S}^2=4\cdot\pi.\eqlbl{eq:area(S2)}\]

Note that the area of a spherical slice $S_\alpha$ between two meridians meeting at angle $\alpha$ is proportional to $\alpha$.
Since for $S_\pi$ is a half-sphere, from \ref{eq:area(S2)}, we get $\area S_\pi\z=2\cdot\pi$.
Therefore the coefficient is 2; that is,
\[\area S_\alpha=2\cdot \alpha.\eqlbl{eq:area(Sa)}\]

Extending the sides of $\Delta$ we get 6 slices: two $S_\alpha$, two $S_\beta$ and two $S_\gamma$ which cover most of the sphere once,
but the triangle $\Delta$ and its centrally symmetric copy $\Delta^{*}$ are covered 3 times.
It follows that
\[2\cdot \area S_\alpha+2\cdot \area S_\beta+2\cdot \area S_\gamma
=\area\mathbb{S}^2+4\cdot\area\Delta.\]
Substituting \ref{eq:area(S2)} and \ref{eq:area(Sa)} and simplifying, we get \ref{eq:area(Delta)}.
\qeds

\subsection*{Schwarz's boot}\label{sec:schwarz-boot}

Recall that we defined length of curve as the exact upper bound on the length of inscribed polygonal lines.
It suggests to define area of a surface as a exact upper bound on the area of polyhedrons inscribed in the surface.

\begin{wrapfigure}{r}{42 mm}
\centering
\includegraphics{asy/schwarz}
\end{wrapfigure}

However as you will see from the following example, this idea fails badly even for cylindrical surface.
Namely, we will show that if we define the area as the least upper bound of areas of inscribed polyhedral surfaces, 
then the area of the lateral surface of the cylinder must be infinite.
The latter contradicts correct intuition that the area of this surface should be the product of the circumference of the base circle and the height of the cylinder.

Let us divide the cylinder into $m$ equal cylinders by planes parallel to its base.
This way we obtain $m+1$ circles on the lateral surface of a cylinder, including both bases.
Further, let us divide each of these circles into $n$ equal arcs in such a way that the dividing points 
of a circle will lie exactly above the midpoints of arcs on the circle under it.
Consider all triangles formed by a chord of such arc and line segments connecting the ends of the chord with the points right above and right below the mid point of its arc.
All the $2\cdot m\cdot n$ equal triangles form a polyhedral surface which is called \index{Schwarz's boot}\emph{Schwarz's boot}.
A Schwarz's boot for $m=8$ and $n=6$ is shown on the diagram.

{

\begin{wrapfigure}{o}{40 mm}
\centering
\includegraphics{mppics/pic-87}
\end{wrapfigure}

Consider one of the triangle $abc$ which form the Schwarz's boot.
By construction, its base $ac$ lies in a horizontal plane and the projection $b^{*}$ of $b$ on this plane bisects the arc $ac$.
Therefore the vertexes of the triangle $ab^{*}c$ are the three consequent vertexes of a regular $2\cdot n$-gone inscribed in a unit circle.
Denote the triangle $ab^{*}c$ by $s_n$; clearly it depends only on $n$ and the radius of the base circle.


Both triangles $abc$ and $ab^{*}c$ are isosceles with shares base $ac$.
Note that the altitude $bp$ is larger than the altitude $b^{*}p$.
Therefore
\begin{align*}
\area [abc]&=\tfrac12\cdot |a-c|\cdot |b-p|>
\\
&>\tfrac12\cdot |a-c|\cdot |b^{*}-p|=
\\
&=\area [ab^{*}c]=
\\
&=s_n.
\end{align*}

}

In particular,
\[S_{m,n}>2\cdot m\cdot n\cdot s_n,\]
where $S_{m,n}$ denotes the total area of the Schwarz's boot.
Consider the pairs $(m,n)$ such that $m$ is much larger than $n$; namely $m>\tfrac1{s_n}$.
Then
\[S_{m,n}> 2\cdot  m\cdot  n\cdot  s_n>2\cdot  \tfrac1{s_n}\cdot  n\cdot  s_n=2\cdot n.\]
Therefore $S_{m,n}\to \infty$ as $n\to \infty$.
