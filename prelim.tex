\chapter{Multivariable calculus}

\section*{Regular value}

A map $\bm{f}\:\RR^m\to\RR^n$ can be thought as an array of functions 
\[f_1,\dots,f_n\:\RR^m\to \RR.\]
The map $\bm{f}$ is called \emph{smooth} if each function $f_i$ is smooth;
that is, all partial derivatives of $f_i$ are defined in the domain of definition of $\bm{f}$.

The Jacobian matrix of $\bm{f}$ at $\bm{x}\in\RR^m$ is defined as
\[\Jac_{\bm{x}}\bm{f}=
\begin{pmatrix}
\dfrac{\partial f_1}{\partial x_1} & \cdots & \dfrac{\partial f_1}{\partial x_m}\\
\vdots & \ddots & \vdots\\
\dfrac{\partial f_n}{\partial x_1} & \cdots & \dfrac{\partial f_n}{\partial x_m} \end{pmatrix};\]
we assume that the right hand side is evaluated at $\bm{x}=(x_1,\dots,x_m)$.

If the Jacobean matrix defines a surjective linear map $\RR^m\to\RR^n$ (that is, if $\rank(\Jac_{\bm{x}}\bm{f})=n$) then we say that 
$\bm{x}$ is a \emph{regular point} of~$\bm{f}$.

If for each point $\bm{x}$ such that $\bm{f}(\bm{x})=\bm{y}$ is regular,
then we say that $\bm{y}$ is a \emph{regular value} of $\bm{f}$.
The following lemma states that most values of a smooth map are regular;
in particular, generic smooth functions satisfy the assumption of the theorem.

\begin{thm}{Sard's lemma}\label{lem:sard}
Given a smooth map $\bm{f}\colon U \z\to \RR^n$ defined on an open set $U\subset \RR^m$, almost all values in $\RR^n$ are regular.
\end{thm}

The words \emph{almost all} means all values, with the possible exception of a set of zero Lebesgue measure.
In particular if one chooses a random value equidistributed in an arbitrarily small ball $B\subset \RR^n$, then it is a regular value of $\bm{f}$ with probability 1.

Note that if $m<n$, then $\bm{f}$ has no regular points.
Therefore the only regular value of $\bm{f}$ are the points in the complement of the image $\Im \bm{f}$.
The theorem states that in this case almost all points in $\RR^n$, do \emph{not} belong to $\Im \bm{f}$.


\section*{Inverse function theorem}

The \emph{inverse function theorem} gives a sufficient condition for a smooth map $\bm{f}$ to be invertible in a neighborhood of a given point $\bm{x}$ in its domain.
The condition is formulated in terms of the Jacobian matrix of $\bm{f}$ at $\bm{x}$.

The \emph{implicit function theorem} is a close relative to the inverse function theorem;
in fact it can be obtained as its corollary.
It is used when we need to pass from parametric to implicit description of curves and surfaces.

Both theorems reduce the existence of a map satisfying certain equation to a question in linear algebra.
We use these two theorems only for $n\le 3$.

These two theorems are discussed in any course of multivariable calculus, the classical book of Walter Rudin \cite{rudin} is one of my favorites.

\begin{thm}{Inverse function theorem}\label{thm:inverse}
Let $\bm{f}=(f_1,\dots,f_n)\:\RR^n\to\RR^n$ be a smooth map.
Assume that the Jacobian matrix
$\Jac_{\bm{x}}\bm{f}$
is invertible at some point $\bm{x}$ in the domain of definition of $\bm{f}$.
Then there is a smooth map $\bm{h}\:\RR^n\to\RR^n$ defined in a neighborhood $\Theta$ of ${\bm{y}}=\bm{f}(\bm{x})$ that is a \emph{local inverse of $\bm{f}$ at $\bm{x}$};
that is, there is a neighborhood $\Omega\ni \bm{x}$ such that
$\bm{f}$ defines a bijection $\Omega\to \Theta$ and
$\bm{h} \circ \bm{f}$ is an identity map on $\Omega$.
\end{thm}

\begin{thm}{Implicit function theorem}\label{thm:imlicit}
Let $\bm{f}=(f_1,\dots,f_n)\:\RR^{n+m}\to\RR^n$ be a smooth map,
$m,n\ge 1$.
Let us consider $\RR^{n+m}$ as a product space $\RR^n\times \RR^m$ with coodinates 
$x_1,\dots,x_n,y_1,\dots,y_m$.
Consider the following matrix 
\[
M=\begin{pmatrix}
\dfrac{\partial f_1}{\partial x_1} & \cdots & \dfrac{\partial f_1}{\partial x_n}\\
\vdots & \ddots & \vdots\\
\dfrac{\partial f_n}{\partial x_1} & \cdots & \dfrac{\partial f_n}{\partial x_n} \end{pmatrix}\]
formed by the first $n$ columns of the Jacobian matrix.
Assume $M$ is invertible at some point $\bm{x}=(x_1,\dots,x_n,y_1,\dots y_m)$ in the domain of definition of $\bm{f}$ and $\bm{f}(\bm{x})=0$.
Then there is a neighborhood $\Omega\ni \bm{x}$
and a smooth function $\bm{h}\:\RR^m\to\RR^n$ defined in a neighborhood $\Theta\ni 0$ such that
for any $(x_1,\dots,x_n,y_1,\dots y_m)\in \Omega$, the equality
\[\bm{f}(x_1,\dots,x_n,y_1,\dots y_m)=0\]
holds if and only if 
\[(x_1,\dots x_n)=\bm{h}(y_1,\dots y_m).\]

\end{thm}

\section*{Multiple integral}

Set 
\[\jac_{\bm{x}}\bm{f}\df|\det[\Jac_{\bm{x}}\bm{f}]|;\]
that is, $\jac_{\bm{x}}\bm{f}$ is the absolute value of the determinant of the Jacobian matrix of $\bm{f}$ at $\bm{x}$.

The following theorem plays the role of a substitution rule for multiple variables.

\begin{thm}{Theorem}\label{thm:mult-substitution}
Let $K\subset \RR^n$ be a compact set and $h\:K\to\RR$ be a bounded measurable function.
Assume $\bm{f}\:K\to \RR^n$ is an injective smooth map.
Then 
\[\int_{\bm{x}\in K} h(\bm{x})\cdot \jac_{\bm{x}}\bm{f}
=
\int_{\bm{y}\in \bm{f}(K)} h\circ \bm{f}^{-1}(\bm{y}).\]

\end{thm}

\section*{Divergence theorem}


Consider a smooth vector field $\vec u$ defined on a domain $\Dom \vec u$ in $\RR^3$.
Recall that divergence of $\vec u$ is defined as $\div\vec u=\langle \nabla,\vec u\rangle$.
In other words, if $\vec i$, $\vec j$, and $\vec k$ denote the elements of the standard basis of  $\RR^3$ and
$\vec u=P\cdot \vec i+Q\cdot\vec j+R\cdot\vec k$
for some smooth functions $P,Q,R\:\Dom \vec u\to \RR$,
then
\[\div\vec u=\tfrac{\partial P}{\partial x}+\tfrac{\partial Q}{\partial y}+\tfrac{\partial R}{\partial z}.\]

\begin{thm}{Theorem}\label{thm:div}
If a piecewise smooth surface $\Sigma$ bounds a body $V$ in $\RR^3$, then
\[\flux_{\vec u}\Sigma=\int_V\div \vec u,\]
assuming that the orientation on $\Sigma$ is defined by a unit normal field that points out of $V$.
\end{thm}


\section*{Curl theorem}

Given a vector field $\vec u$, set $\curl \vec u\df\nabla\times \vec u$;
that is, if we write the field $\vec u$ in the standard basis 
\[\vec u=P\cdot \vec i+ Q\cdot\vec j+R\cdot \vec j,\]
then 
\[\curl \vec u\df \left(\tfrac{\partial R}{\partial y}-\tfrac{\partial Q}{\partial z} \right)\cdot \vec i 
+\left(\tfrac{\partial P}{\partial z}-\tfrac{\partial R}{\partial x}\right) \cdot \vec j +\left (\tfrac{\partial Q}{\partial x}-\tfrac{\partial P}{\partial y}\right)\cdot \vec k \]

\begin{thm}{Theorem}\label{thm:curl}
Let $\Sigma$ be a compact oriented surface bounded by a curve $\gamma$.
Assume that $\gamma\:[0,\ell]\to \RR^3$ is parameterized by its arc-length and oriented in such a way that $\Sigma$ lies on left from it.
Then, for any smooth vector field $\vec u$ defined in a neighborhood of $\Sigma$, we have
\[\flux_{\curl\vec u}\Sigma=\int_0^\ell\langle\vec u,\gamma'(t)\rangle\cdot dt.\]

\end{thm}



\chapter{Differential equations}

\section*{Initial value problem}

The following theorem guarantees existence and uniqueness of solutions of an initial value problem
for a system of ordinary differential equations
\[
\begin{cases}
x_1'(t)&=f_1(x_1,\dots,x_n,t),
\\
&\vdots
\\
x_n'(t)&=f_n(x_1,\dots,x_n,t),
\end{cases}
\]
where each $x_i=x_i(t)$ is a real valued function defined on a real interval $\mathbb{I}$
and each $f_i$ is a smooth function defined on $\RR^n\times \mathbb{I}$.

The array of functions $(f_1,\dots,f_n)$ can be considered as one vector-valued function 
$\bm{f}\:\RR^n\times \mathbb{I}\to \RR^n$ and the array $(x_1,\dots,x_n)$ can be considered as a vector  $\bm{x}\in\RR^n$.
Therefore the system can be rewritten as one vector equation 
\[\bm{x}'(t)=\bm{f}(\bm{x}, t).\] 

%??? make it for arbitrary domain
%??? higher order
\begin{thm}{Theorem}\label{thm:ODE}
Suppose $\mathbb{I}$ is a real interval and $\bm{f}\:\RR^n\times\mathbb{I}\to \RR^n$ is a smooth function.
Then for any initial data $\bm{x}(t_0)=\bm{u}$ the differential equation 
\[\bm{x}'(t)=\bm{f}(\bm{x},t)\]
has a unique solution $\bm{x}(t)$ defined at a maximal subinterval $\mathbb{J}$ of $\mathbb{I}$ that contains $t_0$.
Moreover
\begin{enumerate}[(a)]
\item  if $\mathbb{J}\ne \mathbb{I}$ (that is, if an end $a$ of $\mathbb{J}$ lies in the interior of $\mathbb{I}$) then $\bm{x}(t)$ diverges as $t\to a$;
\item  the function $(\bm{u},t_0,t)\mapsto \bm{x}(t)$ is smooth.
\end{enumerate}


\end{thm}

\chapter{Real analysis}

\section*{Lipschitz condition}

Recall that a function $f$ between metric spaces is called Lipschitz if there is a constant $L$ such that 
\[|f(x)-f(y)|\le L\cdot|x-y|\]
for all values $x$ and $y$ in the domain of definition of $f$.
%Although this definition makes sense for maps between metric spaces, we will only use it for real-to-real functions.

\begin{thm}{Rademacher's theorem}\label{thm:rademacher}
Let $f\:[a,b]\to\RR$ be a Lipschitz function.
Then the derivative $f'(x)$ is defined for alomst all $x\in [a,b]$.
Moreover the derivative $f'$ is a bounded measurable function defined almost everywhere in $[a,b]$ and it satisfies the fundamental theorem of calculus; that is, the following identity 
\[f(b)-f(a)=\int_a^b f'(x)\cdot dx,\]
holds if the integral is understood in the sense of Lebesgue.
\end{thm}

The following theorem makes possible to extend many statements about continuous function to measurable functions.

\begin{thm}{Lusin's theorem}\label{thm:lusin}
Let $\phi\:[a,b]\to \RR$ be a measurable function.
Then for any $\eps>0$, there is a continuous function $\psi_\eps\:[a,b]\to \RR$ that coincides with $\phi$ outside of a set of measure at most $\eps$.
Moreover, if $\phi$ is bounded above and/or below by some constants, then we may assume that so is $\psi_\eps$.  
\end{thm}

\section*{Uniform continuity and convergence}

Let $f$ be a real function defined on a real interval.
If  for any $\eps>0$ there is $\delta>0$ such that 
\[|x_1-x_2|_X<\delta\quad\Longrightarrow\quad |f(x_1)-f(x_2)|_Y<\eps,\]
then $f$ is called \emph{uniformly continuous}.

Evidently every uniformly continuous function is continuous;
the converse does not hold.
For example, the function $f(x)=x^2$ is continuous, but not uniformly continuous.

However if $f$ is continuous and defined on a closed interval $[a,b]$, then $f$ is uniformly continuous

If the condition above holds for any function $f_n$ in a sequence and $\delta$ depend solely on $\eps$,
then the sequence $(f_n)$ is called uniformly equicontinuous.
More precisely, 
a sequence of functions $f_n:X\to Y$ is called \emph{uniformly equicontinuous} if 
for any $\eps>0$ there is $\delta>0$ such that 
\[|x_1-x_2|_X<\delta\quad\Longrightarrow\quad |f_n(x_1)-f_n(x_2)|_Y<\eps\]
for any $n$.
Uniform equicontinuity is the condition in one of the most important convergence theorems in analysis. 
We say that a sequence of functions $f_i : X \to Y$ converges uniformly to a function $f_{\infty}: X \to Y$ if for any 
$\varepsilon >0$, there is a natural number $N$ such that for all $n \geq N$, we have $| f_{\infty} (x)- f_n (x) | < \varepsilon$
for all $x  \in X$.

\begin{thm}{Arzel\'{a}-Ascoli Theorem}\label{lem:equicontinuous}
Any uniformly equicontinuous sequence of function $f_n\:[a,b]\to [c,d]$ has a subsequence that converges uniformly to a continuous function. 
\end{thm}

\section*{Cutoffs and mollifiers}

We use few examples of smooth functions that mimic behavior of some model functions.

%???PIC

For example, consider the function $h\:t\mapsto \max\{\,0,t\,\}$ and the following function
\[f(t)=
\begin{cases}
0&\text{if}\ t\le 0,
\\
\frac{t}{e^{1\!/\!t}}&\text{if}\ t> 0.
\end{cases}
\]
Note that $h$ and $f$ behave alike ---
both vanish at $t\le 0$ and grows to infinity for positive $t$.
The function $h$ is not smooth --- its derivative at $0$ is undefined.
Unlike $h$, the function $f$ is smooth.
Indeed, the existence of all derivatives $f^{(n)}(x)$ at $x\ne 0$ is evident and direct calculations show that $f^{(n)}(0)=0$ for all $n$.

Other useful examples of that type are bell function --- a smooth function that is positive in an $\eps$-neighborhood of zero and vanishing outside this neighborhood.
An example of such function can be constructed based using the function $f$ constructed above, say 
\[b_\eps(t)=c\cdot f(\eps^2-t^2);\]
the constant $c$ can be choosen to ensure that $\int b_\eps=1$.

Another useful example is a smooth nondecreasing function that vanish for $t\le -\eps$ and takes value $1$ for any $t\ge \eps$.
For example the following function \label{page:sigma-function}
\[\sigma_\eps(t)=\int_{-\infty}^t b_\eps(x)\cdot dx.\]


\chapter{Topology}

\section*{Jordan's theorem}

We sometimes use the following characterization of homeomorphisms between compact spaces.

\begin{thm}{Theorem}\label{thm:Hausdorff-compact}
A continuous bijection $f$ between compact metric spaces has a continuous inverse;
that is, $f$ is a homeomorphism.
\end{thm}


The first part of the following theorem was proved by Camille Jordan, the second part is due to Arthur Schoenflies.

\begin{thm}{Theorem}\label{thm:jordan}
The complement of any closed simple plane curve $\gamma$ has exactly two connected components. 

Moreover, there is a homeomorphism $h\:\RR^2\to \RR^2$ that maps the unit circle to $\gamma$.
In particular $\gamma$ bounds a topological disc.
\end{thm}

This theorem is known for its simple formulation and quite hard proof.
By now many proofs of this theorem are known.
For the first statement, a very short proof based on a somewhat developed technique is given by Patrick Doyle \cite{doyle},
among elementary proofs, one of my favorites is the proof given by Aleksei Filippov \cite{filippov}.

We use the following smooth analog of this theorem.

\begin{thm}{Theorem}
The complement of any closed simple smooth regular plane curve $\gamma$ has exactly two connected components. 

Moreover the there is a diffeomorphism $h\:\RR^2\to \RR^2$ that maps the unit circle to $\gamma$.
\end{thm}

The proof of this statement is much simpler.
An amusing proof can be found in \cite{chambers-liokumovich}.

\section*{Connectedness}

Recall that a continuous map $\alpha$ from the unit interval $[0,1]$ to a metric space is called a \emph{path}. If $p=\alpha (0)$ and $q = \alpha (1)$, then we say that $\alpha$ connects $p$ to $q$.


A set $X$ in the Euclidean space is called \emph{path connected} if any two points $x,y\in X$ can be connected by a path lying in $X$.

A set $X$ in the Euclidean space is called \emph{connected} if one cannot cover $X$ with two disjoint open sets $V$ and $W$ such that both intersections $X\cap V$ and $X\cap W$ are nonempty.

\begin{thm}{Proposition}
Any path connected set is connected.
Moreover, any open connected set in the Euclidean space or  plane is path connected.
\end{thm}

Given a point $x\in X$, the maximal connected subset of $X$ containing $x$ is called the \emph{connected component} of $x$ in $X$.



\chapter{Elementary geometry}

\begin{thm}{Theorem}\label{thm:sum=(n-2)pi}
The sum of all the internal angles of a simple $n$-gon is $(n-2)\cdot\pi$. 
\end{thm}


\parit{Proof.}
The proof is by induction on $n$.
For $n=3$ it says that sum of internal angles of a triangle is $\pi$, which is assumed to be known.

First let us show that for any $n\ge4$, any $n$-gon has a diagonal that lies inside of it.
Assume this holds true for all polygons with at most $n-1$ vertices.

Fix an $n$-gon $P$, $n\ge4$.
Applying a rotation if necessary, we can assume that all its vertexes have different $x$-coordinates.
Let $v$ be a vertex of $P$ that minimizes the $x$-coordinate;
denote by $u$ and $w$ its adjacent vertexes.
Let us choose the diagonal $uw$ if it lies in $P$.
Otherwise the triangle $\triangle uvw$ contains another vertex of $P$.
Choose a vertex $s$ in the interior of $\triangle uvw$ that maximizes the distance to line $uw$.
Note that the diagonal $vs$ lies in $P$;
if it is not the case, then $vs$ crosses another side $pq$ of $P$, one of the vertices $p$ or $q$ has larger distance to the line and it lies in the interior of $\triangle uvw$ --- a contradiction.

Note that the diagonal divides $P$ into two polygons, say $Q$ and $R$, with smaller number of sides in each, say $k$ and $m$ correspondingly.
Note that 
\[k+m=n+2;
\eqlbl{eq:k+m=n+2}\]
indeed each side of $P$ appears once as a side of $P$ or $Q$ plus the diagonal appears twice --- once as a side in $Q$ and once as a side of $R$.
Note that the sum of the angles of $P$ is the sum of the angles of $Q$ and $R$, which by the induction hypothesis are $(k-2)\cdot\pi$ and $(m-2)\cdot\pi$ correspondingly.
It remains to note that \ref{eq:k+m=n+2} implies
\[(k-2)\cdot\pi+(m-2)\cdot\pi=(n-2)\cdot\pi.\]
\qedsf

\section*{Triangle inequality for angles}

The following theorem says that the triangle inequality holds for angles between half-lines from a fixed point.
In particular it implies that a sphere with the angle metric is a metric space.

\begin{thm}{Theorem}\label{thm:spherical-triangle-inq}
The inequality
\[\measuredangle aob+\measuredangle boc\ge\measuredangle aoc\]
holds for any three half-lines $oa$, $ob$ and $oc$ in the Euclidean space.
\end{thm}

The following lemma says that the angle of a triangle monotonically depends on the opposit side, assuming the we keep the other two sides fixed. It follows directly from the cosine rule.

\begin{thm}{Lemma}\label{lem:angle-monotonicity}
Let $x$, $y$, $z$, $x'$, $y'$ and $z'$ be 6 points such that $|x-y|=|x'-y'|>0$ and $|y-z|=|y'-z'|>0$.
Then 
\[\measuredangle xyz\ge \measuredangle x'y'z'
\quad\text{if and only if}\quad
|x-z|\ge |x'-z'|.\]
\end{thm}

\begin{wrapfigure}{o}{30 mm}
\vskip-0mm
\centering
\includegraphics{mppics/pic-69}
\vskip0mm
\end{wrapfigure}

\parit{Proof of \ref{thm:spherical-triangle-inq}.}
We can assume that $\measuredangle aob<\measuredangle aoc$; otherwise the statement is evident.
In this case there is a half-line $ob'$ in the angle $aoc$ such that 
\[\measuredangle aob=\measuredangle aob',\]
so in particular we have that
\[\measuredangle aob'+\measuredangle b'oc=\measuredangle aoc.\]
Without loss of generality we can assume that  $|o-b|=|o-b'|$ and $b'$ lies on a line segment $ac$, so
\[|a-b'|+|b'-c|=|a-c|.\]

Then by the triangle inequality 
\[
\begin{aligned}
|a-b|+|b-c|&\ge |a-c|=
\\
&=|a-b'|+|b'-c|.
\end{aligned}
\eqlbl{eq:triangle-inq-abcb'}
\]

Note that in the triangles $aob$ and $aob'$ the side $ao$ is shared, so $\measuredangle aob\z=\measuredangle aob'$ and $|o-b|=|o-b'|$.
By side-angle-side congruence condition, we have that $\triangle aob\cong \triangle aob'$;
in particular $|a-b'|=|a-b|$.
Therefore from \ref{eq:triangle-inq-abcb'} we get 
\[|b-c|\ge |b'-c|.\]
Applying the angle monotonicity (\ref{lem:angle-monotonicity}) we obtain
\[\measuredangle boc\ge \measuredangle b'oc.\]
Whence
\begin{align*}
\measuredangle aob+\measuredangle boc
&\ge \measuredangle aob'+\measuredangle b'oc=
\\
&=\measuredangle aoc.
\end{align*}
\qedsf

\section*{Convexity}

A set $X$ in the Euclidean space is called \emph{convex} if for any two points $x,y\in X$, any point $z$ between $x$ and $y$ lies in $X$.
It is called  \emph{strictly convex} if for any two points $x,y\in X$, any point $z$ between $x$ and $y$ lies in the interior of $X$.

From the definition, it is easy to see that the intersection of an arbitrary family of convex sets is convex. 
The intersection of all convex sets containing $X$ is called the \emph{convex hull} of $X$;
it is the minimal convex set containing the set $X$.

We will use the following corollary of the so called \emph{hyperplane separation theorem}:

\begin{thm}{Lemma}\label{lem:separation}
Let $K\subset \RR^3$ be a closed convex set.
Then for any point $p\notin K$ there is a plane $\Pi$ that separates $K$ from $p$;
that is, $K$ and $p$ lie on opposite open half-spaces separated by $\Pi$.
\end{thm}

A function of two variables $(x,y)\mapsto f(x,y)$ is called convex if 
its epigraph $z\ge f(x,y)$ is a convex set.
This is equivalent to the so-called \emph{Jensen's inequality}
\[f \left (t\cdot x_1 + (1-t)\cdot x_2 \right ) \leq t\cdot f(x_1)+ (1-t)\cdot f(x_2)\]
for $t\in[0,1]$.
If $f$ is smooth, then the condition is equivalent to the following inequality for the second directional derivative:
\[D^2_wf\ge 0\]
for any vector $w\ne 0$ in the $(x,y)$-plane.

\chapter{Area}

\section*{Area of spherical triangle}

\begin{thm}{Lemma}\label{lem:area-spher-triangle}
Let $\Delta$ be a spherical triangle;
that is, $\Delta$ is the intersection of three closed half-spheres in the unit sphere $\SS^2$.
Then 
\[\area\Delta=\alpha+\beta+\gamma-\pi,\eqlbl{eq:area(Delta)}\]
where $\alpha$, $\beta$ and $\gamma$ are the angles of $\Delta$.
\end{thm}

The value $\alpha+\beta+\gamma-\pi$ is called \emph{excess} of the triangle $\Delta$.

\begin{wrapfigure}{o}{22 mm}
\vskip-0mm
\centering
\includegraphics{mppics/pic-43}
\vskip-0mm
\end{wrapfigure}

\parit{Proof.}
Recall that 
\[\area\SS^2=4\cdot\pi.\eqlbl{eq:area(S2)}\]

Note that the area of a spherical slice $S_\alpha$ between two meridians meeting at angle $\alpha$ is proportional to $\alpha$.
Since for $S_\pi$ is a half-sphere, from \ref{eq:area(S2)}, we get $\area S_\pi\z=2\cdot\pi$.
Therefore the coefficient is 2; that is,
\[\area S_\alpha=2\cdot \alpha.\eqlbl{eq:area(Sa)}\]

Extending the sides of $\Delta$ we get 6 slices: two $S_\alpha$, two $S_\beta$ and two $S_\gamma$ which cover most of the sphere once,
but the triangle $\Delta$ and its centrally symmetric copy $\Delta'$ are covered 3 times.
It follows that
\[2\cdot \area S_\alpha+2\cdot \area S_\beta+2\cdot \area S_\gamma
=\area\SS^2+4\cdot\area\Delta.\]
Substituting \ref{eq:area(S2)} and \ref{eq:area(Sa)} and simplifying, we get \ref{eq:area(Delta)}.
\qeds

\section*{Schwarz's boot}\label{sec:schwarz-boot}

Recall that we defined length of curve as the exact upper bound on the length of inscribed polygonal lines.
It suggests to define area of a surface as a exact upper bound on the area of polyhedrons inscribed in the surface.

However as you will see from the following example, this idea fails badly even for cylindrical surface.
Namely, we will show that if we define the area as the least upper bound of areas of inscribed polyhedral surfaces, 
then the area of the lateral surface of the cylinder must be infinite.
The latter contradicts correct intuition that the area of this surface should be the product of the circumference of the base circle and the height of the cylinder.


\begin{wrapfigure}{r}{42 mm}
\centering
\includegraphics{asy/schwarz}
\end{wrapfigure}

Let us divide the cylinder into $m$ equal cylinders by planes parallel to its base.
This way we obtain $m+1$ circles on the lateral surface of a cylinder, including both bases.
Further, let us divide each of these circles into $n$ equal arcs in such a way that the dividing points 
of a circle will lie exactly above the midpoints of arcs on the circle under it.
Consider all triangles formed by a chord of such arc and line segments connecting the ends of the chord with the points right above and right below the mid point of its arc.
All the $2mn$ equal triangles form a polyhedral surface which is called Schwarz's boot.
A Schwarz's boot for $m=8$ and $n=6$ is shown on the diagram.

{

\begin{wrapfigure}{o}{40 mm}
\centering
\includegraphics{mppics/pic-87}
\end{wrapfigure}

Consider one of the triangle $abc$ which form the Schwarz's boot.
By construction, its base $ac$ lies in a horizontal plane and the projection $b'$ of $b$ on this plane bisects the arc $ac$.
Therefore the vertexes of the triangle $ab'c$ are the three consequent vertexes of a regular $2n$-gone inscribed in a unit circle.
Denote the triangle $ab'c$ by $s_n$; clearly it depends only on $n$ and the radius of the base.


Both triangles $abc$ and $ab'c$ are isosceles with shares base $ac$.
Note that the altitude $bp$ is larger than the altitude $b'p$.
Therefore
\begin{align*}
\area (\triangle abc)&=\tfrac12\cdot |a-c|\cdot |b-p|>
\\
&>\tfrac12\cdot |a-c|\cdot |b'-p|=
\\
&=\area (\triangle ab'c)=
\\
&=s_n.
\end{align*}

}

In particular,
\[S_{m,n}>2\cdot m\cdot n\cdot s_n,\]
where $S_{m,n}$ denotes the total area of the Schwarz's boot.
Consider the pairs $(m,n)$ such that $m$ is much larger than $n$; namely $m>\tfrac1{s_n}$.
Then
\[S_{m,n}> 2\cdot  m\cdot  n\cdot  s_n>2\cdot  \tfrac1{s_n}\cdot  n\cdot  s_n=2\cdot n.\]
Therefore $S_{m,n}\to \infty$ as $n\to \infty$.
